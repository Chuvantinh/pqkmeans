{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: PQk-means\n",
    "This chapter contains the followings:\n",
    "\n",
    "1. Vector compression by Product Quantization\n",
    "1. Clustering by PQk-means\n",
    "1. Comparison to other clustering methods\n",
    "\n",
    "Requisites:\n",
    "- numpy\n",
    "- sklearn\n",
    "- pqkmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vector compression by Product Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pqkmeans\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First , we introduce vector compression by Product Quantization (PQ) [Jegou, TPAMI 11]. The first task is to train an encoder. Let us assume that there are 1000 six-dimensional vectors for training; $X_1 \\in \\mathbb{R}^{1000\\times6}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape =  \n",
      " (1000, 6) \n",
      "\n",
      "X1 = \n",
      " [[ 0.41924871  0.39184232  0.25803356  0.65040351  0.21106254  0.72738996]\n",
      " [ 0.33881997  0.4123091   0.69142687  0.45186094  0.19894196  0.06499685]\n",
      " [ 0.47682624  0.43083143  0.38538358  0.1519837   0.60178826  0.54658913]\n",
      " ..., \n",
      " [ 0.11577582  0.43086611  0.27831923  0.60215751  0.62279282  0.65356463]\n",
      " [ 0.20633001  0.09706879  0.26794791  0.0876635   0.42875445  0.50787089]\n",
      " [ 0.60334538  0.41343645  0.10305301  0.34764159  0.16666211  0.69682394]]\n"
     ]
    }
   ],
   "source": [
    "X1 = numpy.random.random((1000, 6))\n",
    "print(\"X1.shape =  \\n\", X1.shape, \"\\n\")\n",
    "print(\"X1 = \\n\", X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train a PQEncoder using $X_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = pqkmeans.encoder.PQEncoder(num_subdim=2, Ks=256)\n",
    "encoder.fit(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes two parameters: $num\\_subdim$ and $Ks$. In the training step, each vector is splitted into $num\\_subdim$ sub-vectors, and quantized with $Ks$ codewords. The $num\\_subdim$ decides the bit length of PQ-code, and typically set as 4, 8, etc. The $Ks$ is usually set as 256.\n",
    "\n",
    "In this example, each 6D training vector is splitted into $num\\_subdim(=2)$ sub-vectors (two 3D vectors). Consequently, the 1000 6D training vectors are splitted into the two set of 1000 3D vectors. The k-means clustering is applied for each set of subvectors with $K=256$.\n",
    "\n",
    "\n",
    "After the training step, the encoder stores the resulting codewords (2 subpspaces $*$ 256 codewords $*$ 3 dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(encoder.codewords.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can train the encoder preliminary using training data, and write/read the encoder via pickle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(encoder, open('encoder.pkl', 'wb'))  # Write\n",
    "# encoder = pickle.load(open('encoder.pkl', 'rb'))  # Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us consider database vectors (2000 six-dimensional vectors, $X_2$) that we'd like to compress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2.shape:\n",
      " (2000, 6) \n",
      "\n",
      "X2:\n",
      " [[ 0.69521066  0.473434    0.81014451  0.83199628  0.53207691  0.68057145]\n",
      " [ 0.92873026  0.79807992  0.4145905   0.32912862  0.01921358  0.30650001]\n",
      " [ 0.71014149  0.10309274  0.99991603  0.49397926  0.66469458  0.32774237]\n",
      " ..., \n",
      " [ 0.29750077  0.53425779  0.62608256  0.0462519   0.99887679  0.85922414]\n",
      " [ 0.45633923  0.88007103  0.89685826  0.50982455  0.15277933  0.84326488]\n",
      " [ 0.55456954  0.556289    0.86725168  0.72495792  0.8088668   0.36371054]] \n",
      "\n",
      "Data type of each element:\n",
      " <class 'numpy.float64'> \n",
      "\n",
      "Memory usage:\n",
      " 96000 byte\n"
     ]
    }
   ],
   "source": [
    "X2 = numpy.random.random((2000, 6))\n",
    "print(\"X2.shape:\\n\", X2.shape, \"\\n\")\n",
    "print(\"X2:\\n\", X2, \"\\n\")\n",
    "print(\"Data type of each element:\\n\", type(X2[0][0]), \"\\n\")\n",
    "print(\"Memory usage:\\n\", X2.nbytes, \"byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compress these vectors by the trained PQ-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2_pqcode.shape:\n",
      " (2000, 2) \n",
      "\n",
      "X2_pqcode\n",
      " [[ 91  10]\n",
      " [ 16 209]\n",
      " [125 225]\n",
      " ..., \n",
      " [203  20]\n",
      " [114  97]\n",
      " [226 153]] \n",
      "\n",
      "Data type of each element:\n",
      " <class 'numpy.uint8'> \n",
      "\n",
      "Memory usage:\n",
      " 4000 byte\n"
     ]
    }
   ],
   "source": [
    "X2_pqcode = encoder.transform(X2)\n",
    "print(\"X2_pqcode.shape:\\n\", X2_pqcode.shape, \"\\n\")\n",
    "print(\"X2_pqcode\\n\", X2_pqcode, \"\\n\")\n",
    "print(\"Data type of each element:\\n\", type(X2_pqcode[0][0]), \"\\n\")\n",
    "print(\"Memory usage:\\n\", X2_pqcode.nbytes, \"byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector is splitted into $num\\_subdim(=2)$ sub-vectors, and the nearest codeword is searched for each sub-vector. The id of the nearest codeword is recorded, i.e., two integers in this case. This representation is called PQ-code.\n",
    " \n",
    "Note that PQ-code is a mamemory efficient data representation. The original 6D vector requies $6 * 64 = 384$ bit if 64 bit float is used for each element. On the other, PQ code requires only $2 * \\log_2 256 = 16$ bit. \n",
    "\n",
    "We can approximately recunstruct the original vector from a PQ-code, by fetching the codewords using the PQ-code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original X2:\n",
      " [[ 0.69521066  0.473434    0.81014451  0.83199628  0.53207691  0.68057145]\n",
      " [ 0.92873026  0.79807992  0.4145905   0.32912862  0.01921358  0.30650001]\n",
      " [ 0.71014149  0.10309274  0.99991603  0.49397926  0.66469458  0.32774237]\n",
      " ..., \n",
      " [ 0.29750077  0.53425779  0.62608256  0.0462519   0.99887679  0.85922414]\n",
      " [ 0.45633923  0.88007103  0.89685826  0.50982455  0.15277933  0.84326488]\n",
      " [ 0.55456954  0.556289    0.86725168  0.72495792  0.8088668   0.36371054]] \n",
      "\n",
      "reconstructed X2:\n",
      " [[ 0.7305386   0.41790788  0.91171652  0.79356276  0.55726669  0.63313401]\n",
      " [ 0.93921649  0.84747755  0.49089569  0.32181604  0.06143173  0.2127218 ]\n",
      " [ 0.67338156  0.09242146  0.95811486  0.53955805  0.57155243  0.27144563]\n",
      " ..., \n",
      " [ 0.34576532  0.5893726   0.53532704  0.07156711  0.94468507  0.86145908]\n",
      " [ 0.42363968  0.85768321  0.81125923  0.47217088  0.16601343  0.72239844]\n",
      " [ 0.47092851  0.58453229  0.88074799  0.77164221  0.82516589  0.37545323]]\n"
     ]
    }
   ],
   "source": [
    "X2_reconstructed = encoder.inverse_transform(X2_pqcode)\n",
    "print(\"original X2:\\n\", X2, \"\\n\")\n",
    "print(\"reconstructed X2:\\n\", X2_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the reconstructed vectors are similar to the original one.\n",
    "\n",
    "In a large-scale data processing scenario where all data cannot be stored on memory, you can compress input vectors to PQ-codes and store the PQ-codes only (X2_pqcode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(X2_pqcode, open('pqcode.pkl', 'wb')) # You can store the PQ-codes only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering by PQk-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the clustering over the PQ-codes. The clustering object is instanciated with the trained encoder. Here, we set the number of cluster as $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = pqkmeans.clustering.PQKMeans(encoder=encoder, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the PQk-means over X2_pqcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 8, 0, 7, 9, 7, 6, 1, 6, 2, 4, 9, 4, 4, 7, 7, 2, 7, 5, 2, 7, 0, 6, 7, 6, 2, 6, 0, 8, 2, 1, 9, 9, 5, 8, 5, 9, 2, 6, 8, 7, 6, 6, 5, 1, 5, 4, 7, 7, 4, 4, 8, 9, 9, 7, 3, 2, 6, 0, 8, 8, 5, 9, 8, 2, 8, 9, 5, 8, 8, 6, 5, 1, 7, 3, 5, 8, 0, 0, 7, 3, 4, 9, 8, 4, 0, 2, 4, 1, 5, 2, 9, 8, 5, 4, 0, 1, 4, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "clustered = kmeans.fit_predict(X2_pqcode)\n",
    "print(clustered[:100]) # Just show the 100 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector (clustered) contains the id of assigned codeword for each input PQ-code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The id of assigned codeword for the 1st PQ-code is  5\n",
      "The id of assigned codeword for the 2nd PQ-code is  8\n",
      "The id of assigned codeword for the 3rd PQ-code is  0\n"
     ]
    }
   ],
   "source": [
    "print(\"The id of assigned codeword for the 1st PQ-code is \", clustered[0])\n",
    "print(\"The id of assigned codeword for the 2nd PQ-code is \", clustered[1])\n",
    "print(\"The id of assigned codeword for the 3rd PQ-code is \", clustered[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can fetch the center of the clustering by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering centers:\n",
      " [[51, 248], [249, 197], [107, 249], [154, 49], [14, 16], [65, 165], [55, 148], [146, 225], [211, 55], [175, 159]]\n"
     ]
    }
   ],
   "source": [
    "print(\"clustering centers:\\n\", kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centers are also PQ-codes. They can be reconstructed by PQ-encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructed clustering centers:\n",
      " [[ 0.34063962  0.46187204  0.94672412  0.63280575  0.44489968  0.229906  ]\n",
      " [ 0.59851806  0.59250557  0.23761559  0.37800664  0.65305798  0.39798818]\n",
      " [ 0.28276182  0.49910144  0.31672088  0.67482735  0.34874969  0.76095219]\n",
      " [ 0.44099195  0.221538    0.83921304  0.62238899  0.28546003  0.70339381]\n",
      " [ 0.86247083  0.1914981   0.3646877   0.57631009  0.2564538   0.28428649]\n",
      " [ 0.63263658  0.66467431  0.69727081  0.64420818  0.70553312  0.76480393]\n",
      " [ 0.24648559  0.77371957  0.40618459  0.44454695  0.49833222  0.4897001 ]\n",
      " [ 0.39258137  0.28525802  0.41045738  0.53955805  0.57155243  0.27144563]\n",
      " [ 0.73492665  0.64110944  0.52159744  0.34383123  0.26950374  0.52341666]\n",
      " [ 0.50715649  0.33016919  0.63211549  0.15936672  0.64721403  0.75495636]]\n"
     ]
    }
   ],
   "source": [
    "clustering_centers_numpy = numpy.array(kmeans.cluster_centers_, dtype=encoder.code_dtype)  # Convert to np.array with the proper dtype\n",
    "clustering_centers_reconstructd = encoder.inverse_transform(clustering_centers_numpy) # From PQ-code to 6D vectors\n",
    "print(\"reconstructed clustering centers:\\n\", clustering_centers_reconstructd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summalize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13th input vector:\n",
      " [ 0.87634215  0.2022789   0.24760898  0.41616456  0.37486582  0.33386463] \n",
      "\n",
      "13th PQ code:\n",
      " [ 20 203] \n",
      "\n",
      "reconstructed 13th PQ code:\n",
      " [ 0.96218507  0.2206644   0.3197819   0.44805649  0.37104167  0.39023411] \n",
      "\n",
      "ID of the assigned center:\n",
      " 4 \n",
      "\n",
      "Assigned center (PQ-code):\n",
      " [14, 16] \n",
      "\n",
      "Assigned center (reconstructed):\n",
      " [ 0.86247083  0.1914981   0.3646877   0.57631009  0.2564538   0.28428649]\n"
     ]
    }
   ],
   "source": [
    "print(\"13th input vector:\\n\", X2[12], \"\\n\")\n",
    "print(\"13th PQ code:\\n\", X2_pqcode[12], \"\\n\")\n",
    "print(\"reconstructed 13th PQ code:\\n\", X2_reconstructed[12], \"\\n\")\n",
    "print(\"ID of the assigned center:\\n\", clustered[12], \"\\n\")\n",
    "print(\"Assigned center (PQ-code):\\n\", kmeans.cluster_centers_[clustered[12]], \"\\n\")\n",
    "print(\"Assigned center (reconstructed):\\n\", clustering_centers_reconstructd[clustered[12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison to other clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare PQk-means and the traditional k-means using high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3 = numpy.random.random((1000, 1024))  # 1K 1024-dim vectors, for training \n",
    "X4 = numpy.random.random((10000, 1024)) # 10K 1024-dim vectors, for database\n",
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the encoder\n",
    "encoder_large = pqkmeans.encoder.PQEncoder(num_subdim=4, Ks=256)\n",
    "encoder_large.fit(X3)\n",
    "\n",
    "# Encode the vectors to PQ-code\n",
    "X4_pqcode = encoder_large.transform(X4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the PQ-kmeans, and see the computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 328 ms, sys: 0 ns, total: 328 ms\n",
      "Wall time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "%time clustered_pqkmeans = pqkmeans.clustering.PQKMeans(encoder=encoder_large, k=K).fit_predict(X4_pqcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the traditional k-means clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 s, sys: 40 ms, total: 1.99 s\n",
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%time clustered_kmeans = KMeans(n_clusters=K, n_jobs=-1).fit_predict(X4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PQk-means would be tens to hundreds of times faster than k-means depending on your machine. Then let's see the accuracy. Since the result of PQk-means is the approximation of that of k-means, k-means achieved the lower error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQk-means, micro avg error:  9.17477287633\n",
      "k-means, micro avg error:  9.1501990608\n"
     ]
    }
   ],
   "source": [
    "_, pqkmeans_micro_average_error, _ = pqkmeans.evaluation.calc_error(clustered_pqkmeans, X4, K)\n",
    "_, kmeans_micro_average_error, _ = pqkmeans.evaluation.calc_error(clustered_kmeans, X4, K)\n",
    "\n",
    "print(\"PQk-means, micro avg error: \", pqkmeans_micro_average_error)\n",
    "print(\"k-means, micro avg error: \", kmeans_micro_average_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
