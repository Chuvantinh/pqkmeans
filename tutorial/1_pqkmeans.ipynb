{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: PQk-means\n",
    "This chapter contains the followings:\n",
    "\n",
    "1. Vector compression by Product Quantization\n",
    "1. Clustering by PQk-means\n",
    "1. Comparison to other clustering methods\n",
    "\n",
    "Requisites:\n",
    "- numpy\n",
    "- sklearn\n",
    "- pqkmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vector compression by Product Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pqkmeans\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First , we introduce vector compression by Product Quantization (PQ) [Jegou, TPAMI 11]. The first task is to train an encoder. Let us assume that there are 1000 six-dimensional vectors for training; $X_1 \\in \\mathbb{R}^{1000\\times6}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape =  \n",
      " (1000, 6) \n",
      "\n",
      "X1 = \n",
      " [[ 0.70275556  0.51219496  0.76284138  0.13980293  0.86790663  0.59893453]\n",
      " [ 0.37008827  0.39502803  0.73402674  0.43158098  0.79462144  0.96854116]\n",
      " [ 0.24389028  0.78572064  0.15534625  0.19633922  0.90830083  0.42978262]\n",
      " ..., \n",
      " [ 0.78156807  0.04260586  0.50256526  0.82012482  0.73211878  0.76370338]\n",
      " [ 0.76902753  0.2173942   0.74703122  0.47439234  0.74531087  0.30878455]\n",
      " [ 0.21489385  0.13756352  0.59329169  0.92189439  0.80937742  0.06592726]]\n"
     ]
    }
   ],
   "source": [
    "X1 = numpy.random.random((1000, 6))\n",
    "print(\"X1.shape =  \\n\", X1.shape, \"\\n\")\n",
    "print(\"X1 = \\n\", X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train a PQEncoder using $X_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = pqkmeans.encoder.PQEncoder(num_subdim=2, Ks=256)\n",
    "encoder.fit(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes two parameters: $num\\_subdim$ and $Ks$. In the training step, each vector is splitted into $num\\_subdim$ sub-vectors, and quantized with $Ks$ codewords. The $num\\_subdim$ decides the bit length of PQ-code, and typically set as 4, 8, etc. The $Ks$ is usually set as 256.\n",
    "\n",
    "In this example, each 6D training vector is splitted into $num\\_subdim(=2)$ sub-vectors (two 3D vectors). Consequently, the 1000 6D training vectors are splitted into the two set of 1000 3D vectors. The k-means clustering is applied for each set of subvectors with $K=256$.\n",
    "\n",
    "\n",
    "After the training step, the encoder stores the resulting codewords (2 subpspaces $*$ 256 codewords $*$ 3 dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(encoder.codewords.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can train the encoder preliminary using training data, and write/read the encoder via pickle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(encoder, open('encoder.pkl', 'wb'))  # Write\n",
    "# encoder = pickle.load(open('encoder.pkl', 'rb'))  # Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us consider database vectors (2000 six-dimensional vectors, $X_2$) that we'd like to compress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2.shape:\n",
      " (2000, 6) \n",
      "\n",
      "X2:\n",
      " [[ 0.0156      0.01311389  0.35813774  0.3859867   0.21378337  0.59912353]\n",
      " [ 0.71678096  0.35654127  0.85522785  0.4384428   0.51480571  0.60912294]\n",
      " [ 0.04447281  0.84149141  0.15362455  0.26419114  0.15876926  0.22082696]\n",
      " ..., \n",
      " [ 0.25256687  0.58320653  0.15698569  0.08444892  0.02452599  0.64746556]\n",
      " [ 0.55939827  0.17863138  0.3245326   0.36510014  0.07182216  0.70163746]\n",
      " [ 0.93133231  0.18555859  0.68414746  0.58857661  0.30737436  0.62998395]] \n",
      "\n",
      "Data type of each element:\n",
      " <class 'numpy.float64'> \n",
      "\n",
      "Memory usage:\n",
      " 96000 byte\n"
     ]
    }
   ],
   "source": [
    "X2 = numpy.random.random((2000, 6))\n",
    "print(\"X2.shape:\\n\", X2.shape, \"\\n\")\n",
    "print(\"X2:\\n\", X2, \"\\n\")\n",
    "print(\"Data type of each element:\\n\", type(X2[0][0]), \"\\n\")\n",
    "print(\"Memory usage:\\n\", X2.nbytes, \"byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compress these vectors by the trained PQ-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2_pqcode.shape:\n",
      " (2000, 2) \n",
      "\n",
      "X2_pqcode\n",
      " [[128 109]\n",
      " [215  15]\n",
      " [174  22]\n",
      " ..., \n",
      " [244 206]\n",
      " [125 189]\n",
      " [ 32 105]] \n",
      "\n",
      "Data type of each element:\n",
      " <class 'numpy.uint8'> \n",
      "\n",
      "Memory usage:\n",
      " 4000 byte\n"
     ]
    }
   ],
   "source": [
    "X2_pqcode = encoder.transform(X2)\n",
    "print(\"X2_pqcode.shape:\\n\", X2_pqcode.shape, \"\\n\")\n",
    "print(\"X2_pqcode\\n\", X2_pqcode, \"\\n\")\n",
    "print(\"Data type of each element:\\n\", type(X2_pqcode[0][0]), \"\\n\")\n",
    "print(\"Memory usage:\\n\", X2_pqcode.nbytes, \"byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector is splitted into $num\\_subdim(=2)$ sub-vectors, and the nearest codeword is searched for each sub-vector. The id of the nearest codeword is recorded, i.e., two integers in this case. This representation is called PQ-code.\n",
    " \n",
    "Note that PQ-code is a mamemory efficient data representation. The original 6D vector requies $6 * 64 = 384$ bit if 64 bit float is used for each element. On the other, PQ code requires only $2 * \\log_2 256 = 16$ bit. \n",
    "\n",
    "We can approximately recunstruct the original vector from a PQ-code, by fetching the codewords using the PQ-code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original X2:\n",
      " [[ 0.0156      0.01311389  0.35813774  0.3859867   0.21378337  0.59912353]\n",
      " [ 0.71678096  0.35654127  0.85522785  0.4384428   0.51480571  0.60912294]\n",
      " [ 0.04447281  0.84149141  0.15362455  0.26419114  0.15876926  0.22082696]\n",
      " ..., \n",
      " [ 0.25256687  0.58320653  0.15698569  0.08444892  0.02452599  0.64746556]\n",
      " [ 0.55939827  0.17863138  0.3245326   0.36510014  0.07182216  0.70163746]\n",
      " [ 0.93133231  0.18555859  0.68414746  0.58857661  0.30737436  0.62998395]] \n",
      "\n",
      "reconstructed X2:\n",
      " [[ 0.01975923  0.06736936  0.46318343  0.39555917  0.28400721  0.54631893]\n",
      " [ 0.66183932  0.33606359  0.82841298  0.45640054  0.54916867  0.53466686]\n",
      " [ 0.10339425  0.79627874  0.17225137  0.28756499  0.10818275  0.16972432]\n",
      " ..., \n",
      " [ 0.26826145  0.63171795  0.07940289  0.20309724  0.04658785  0.69167184]\n",
      " [ 0.50138988  0.06807226  0.26473145  0.3341703   0.0377606   0.65188995]\n",
      " [ 0.88204834  0.19634262  0.63536372  0.54954645  0.30228684  0.52894986]]\n"
     ]
    }
   ],
   "source": [
    "X2_reconstructed = encoder.inverse_transform(X2_pqcode)\n",
    "print(\"original X2:\\n\", X2, \"\\n\")\n",
    "print(\"reconstructed X2:\\n\", X2_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the reconstructed vectors are similar to the original one.\n",
    "\n",
    "In a large-scale data processing scenario where all data cannot be stored on memory, you can compress input vectors to PQ-codes and store the PQ-codes only (X2_pqcode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(X2_pqcode, open('pqcode.pkl', 'wb')) # You can store the PQ-codes only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering by PQk-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the clustering over the PQ-codes. The clustering object is instanciated with the trained encoder. Here, we set the number of cluster as $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = pqkmeans.clustering.PQKMeans(encoder=encoder, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the PQk-means over X2_pqcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 0, 0, 8, 4, 2, 2, 6, 4, 0, 6, 1, 0, 1, 3, 5, 7, 3, 5, 7, 8, 1, 2, 2, 0, 3, 7, 5, 4, 4, 3, 8, 5, 1, 4, 1, 7, 2, 6, 7, 4, 7, 4, 9, 4, 6, 1, 3, 6, 0, 5, 9, 5, 8, 6, 4, 4, 0, 3, 6, 4, 3, 3, 0, 0, 4, 8, 3, 3, 2, 8, 0, 9, 7, 4, 6, 0, 7, 0, 9, 0, 6, 8, 7, 2, 9, 3, 2, 3, 5, 5, 1, 3, 4, 2, 7, 2, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "clustered = kmeans.fit_predict(X2_pqcode)\n",
    "print(clustered[:100]) # Just show the 100 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector (clustered) contains the id of assigned codeword for each input PQ-code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The id of assigned codeword for the 1st PQ-code is  6\n",
      "The id of assigned codeword for the 2nd PQ-code is  2\n",
      "The id of assigned codeword for the 3rd PQ-code is  0\n"
     ]
    }
   ],
   "source": [
    "print(\"The id of assigned codeword for the 1st PQ-code is \", clustered[0])\n",
    "print(\"The id of assigned codeword for the 2nd PQ-code is \", clustered[1])\n",
    "print(\"The id of assigned codeword for the 3rd PQ-code is \", clustered[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can fetch the center of the clustering by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering centers:\n",
      " [[170, 92], [24, 232], [152, 146], [126, 6], [150, 128], [171, 96], [163, 106], [237, 254], [120, 134], [193, 112]]\n"
     ]
    }
   ],
   "source": [
    "print(\"clustering centers:\\n\", kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centers are also PQ-codes. They can be reconstructed by PQ-encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructed clustering centers:\n",
      " [[ 0.53360177  0.77639461  0.63012951  0.26380888  0.31202664  0.29219604]\n",
      " [ 0.74692713  0.56350071  0.73656737  0.49674636  0.78332985  0.41896516]\n",
      " [ 0.75852336  0.14173008  0.73176168  0.68377146  0.43976632  0.6449249 ]\n",
      " [ 0.39630225  0.61564376  0.44836144  0.71438811  0.67933583  0.18204068]\n",
      " [ 0.46460275  0.63710803  0.58704328  0.65678022  0.48001491  0.88080196]\n",
      " [ 0.59859426  0.28144411  0.53685033  0.22135294  0.19829786  0.67700851]\n",
      " [ 0.24306931  0.45709692  0.32156575  0.3753507   0.4738097   0.65171996]\n",
      " [ 0.66934827  0.3932376   0.34434023  0.23362457  0.80432322  0.79646046]\n",
      " [ 0.79809461  0.38572955  0.2642597   0.64587247  0.24504116  0.22047964]\n",
      " [ 0.47851439  0.19963551  0.54408934  0.29084716  0.66420591  0.19116989]]\n"
     ]
    }
   ],
   "source": [
    "clustering_centers_numpy = numpy.array(kmeans.cluster_centers_, dtype=encoder.code_dtype)  # Convert to np.array with the proper dtype\n",
    "clustering_centers_reconstructd = encoder.inverse_transform(clustering_centers_numpy) # From PQ-code to 6D vectors\n",
    "print(\"reconstructed clustering centers:\\n\", clustering_centers_reconstructd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summalize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13th input vector:\n",
      " [ 0.99527229  0.42485577  0.69820284  0.62567077  0.84666029  0.70164452] \n",
      "\n",
      "13th PQ code:\n",
      " [ 63 157] \n",
      "\n",
      "reconstructed 13th PQ code:\n",
      " [ 0.97862943  0.32630291  0.73809074  0.62566541  0.90514829  0.62460715] \n",
      "\n",
      "ID of the assigned center:\n",
      " 1 \n",
      "\n",
      "Assigned center (PQ-code):\n",
      " [24, 232] \n",
      "\n",
      "Assigned center (reconstructed):\n",
      " [ 0.74692713  0.56350071  0.73656737  0.49674636  0.78332985  0.41896516]\n"
     ]
    }
   ],
   "source": [
    "print(\"13th input vector:\\n\", X2[12], \"\\n\")\n",
    "print(\"13th PQ code:\\n\", X2_pqcode[12], \"\\n\")\n",
    "print(\"reconstructed 13th PQ code:\\n\", X2_reconstructed[12], \"\\n\")\n",
    "print(\"ID of the assigned center:\\n\", clustered[12], \"\\n\")\n",
    "print(\"Assigned center (PQ-code):\\n\", kmeans.cluster_centers_[clustered[12]], \"\\n\")\n",
    "print(\"Assigned center (reconstructed):\\n\", clustering_centers_reconstructd[clustered[12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison to other clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare PQk-means and the traditional k-means using high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3 = numpy.random.random((1000, 1024))  # 1K 1024-dim vectors, for training \n",
    "X4 = numpy.random.random((10000, 1024)) # 10K 1024-dim vectors, for database\n",
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the encoder\n",
    "encoder_large = pqkmeans.encoder.PQEncoder(num_subdim=4, Ks=256)\n",
    "encoder_large.fit(X3)\n",
    "\n",
    "# Encode the vectors to PQ-code\n",
    "X4_pqcode = encoder_large.transform(X4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the PQ-kmeans, and see the computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 264 ms, sys: 0 ns, total: 264 ms\n",
      "Wall time: 152 ms\n"
     ]
    }
   ],
   "source": [
    "%time clustered_pqkmeans = pqkmeans.clustering.PQKMeans(encoder=encoder_large, k=K).fit_predict(X4_pqcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the traditional k-means clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 s, sys: 68 ms, total: 2.14 s\n",
      "Wall time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "%time clustered_kmeans = KMeans(n_clusters=K, n_jobs=-1).fit_predict(X4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PQk-means would be tens to hundreds of times faster than k-means depending on your machine. Then let's see the accuracy. Since the result of PQk-means is the approximation of that of k-means, k-means achieved the lower error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQk-means, micro avg error:  9.17064329162\n",
      "k-means, micro avg error:  9.1461016175\n"
     ]
    }
   ],
   "source": [
    "_, pqkmeans_micro_average_error, _ = pqkmeans.evaluation.calc_error(clustered_pqkmeans, X4, K)\n",
    "_, kmeans_micro_average_error, _ = pqkmeans.evaluation.calc_error(clustered_kmeans, X4, K)\n",
    "\n",
    "print(\"PQk-means, micro avg error: \", pqkmeans_micro_average_error)\n",
    "print(\"k-means, micro avg error: \", kmeans_micro_average_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
